{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_mean = np.array([0.485,0.456,0.405])\n",
    "rgb_std = np.array([0.229,0.224,0.225])\n",
    "def preprocess(img,image_shape):\n",
    "    process = torchvision.transforms.compose([\n",
    "        torchvision.transforms.Resize(image_shape),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean = rgb_mean,std = rgb_std)       \n",
    "    ])\n",
    "    return process\n",
    "\n",
    "def postprocess(img_tensor):\n",
    "    inv_normalize = torchvision.transforms.Normalize(mean = -rgb_mean / rgb_std, std = 1/rgb_std)\n",
    "    to_PIL_image = torchvision.tranforms.ToPILImage()\n",
    "    return to_PIL_image(inv_normalize(img_tensor[0])).clamp(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_net = torchvision.models.vgg19(pretrained= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_layer,style_layer =[0,5,10,19,28],[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_list = []\n",
    "for i in range(max(content_layer+style_layer)+1):\n",
    "    net_list.append(pretrained_net.features[i])\n",
    "\n",
    "net = nn.Sequential(*net_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(X, content_layer , style_layer):\n",
    "    contents = []\n",
    "    style = []\n",
    "    for i in range(len(net)):\n",
    "        X = net[i](X)\n",
    "        if i in content_layer:\n",
    "            contents.append(X)\n",
    "        if i in style_layer :\n",
    "            style.append(X)\n",
    "\n",
    "    return contents , style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contents(image_shape):\n",
    "    content_X = preprocess(content_img , image_shape)\n",
    "    contents_Y,_ = extract_features(content_X, content_layer, style_layer)\n",
    "    return content_X, contents_Y\n",
    "\n",
    "def get_styles(image_shape):\n",
    "    style_X = prepocess(style_img, image_shape)\n",
    "    _, styles_Y = extract_features(style_X, content_layer, style_layer)\n",
    "    return style_X, style_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram(X):\n",
    "    num_channels , n = X.shape[1],X.shape[2]*X.shape[3]\n",
    "    X = X.view(num_channels , n)\n",
    "    return torch.matmul(X,X.t()) / (num_channels *n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_loss(Y_hat , gram_Y):\n",
    "    return F.mse_loss(gram(Y_hat), gram_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_loss(Y_hat, Y):\n",
    "    return F.mse_loss(Y_hat, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tv_loss(Y_hat):\n",
    "    return 0.5(F.l1_loss(Y_hat[:,:,1:,:],Y_hat[:,:,:-1,:])+\n",
    "                F.l1_loss(Y_hat[:,:,:,1:],Y_hat[:,:,:,:-1])\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_weight , style_weight, tv_weight = 1, 1e3,10\n",
    "\n",
    "def compute_loss(X, contents_Y_hat,styles_Y_hat,contents_Y,styles_Y):\n",
    "    contents_l = [content_loss(Y_hat,Y)* content_weight for Y_hat,Y in zip(contents_Y_hat,contents_Y)\n",
    "                 ]\n",
    "    styles_l = [style_loss(Y_hat, Y)* style_weight for Y_hat,Y in zip(styles_Y_hat,styles_Y)]\n",
    "    tv_l = tv_loss(X)*tv_weight\n",
    "    l = sum(styles_l) + sum(contents_l)+tv_l\n",
    "    return contents_l,styles_l,l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateImage(nn.Module):\n",
    "    def __init__(self , img_shape):\n",
    "        super(GenerateImage,self).__init__()\n",
    "        self.weight = nn.Parameter(torch.rand(*image_shape))\n",
    "\n",
    "    def forward(self):\n",
    "        return self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inits(X, lr, styles_Y):\n",
    "    gen_img  = GenerateImage(X.shape)\n",
    "    gen_img.weight.data = X.data\n",
    "    optimizer = torch.optim.Adam(gen_img.parameters(),lr =lr)\n",
    "    style_Y_gram = [gram(Y) for Y in styles_Y]\n",
    "    return gen_img(), styles_Y_gram, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, contents_Y, styles_Y, device, lr, max_epochs, lr_decay_epoch):\n",
    "    print(\"training on \", device)\n",
    "    X, styles_Y_gram, optimizer = get_inits(X, device, lr, styles_Y)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, lr_decay_epoch, gamma=0.1)\n",
    "    for i in range(max_epochs):\n",
    "        start = time.time()\n",
    "        \n",
    "        contents_Y_hat, styles_Y_hat = extract_features(\n",
    "                X, content_layers, style_layers)\n",
    "        contents_l, styles_l, tv_l, l = compute_loss(\n",
    "                X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        l.backward(retain_graph = True)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        if i % 50 == 0 and i != 0:\n",
    "            print('epoch %3d, content loss %.2f, style loss %.2f, '\n",
    "                  'TV loss %.2f, %.2f sec'\n",
    "                  % (i, sum(contents_l).item(), sum(styles_l).item(), tv_l.item(),\n",
    "                     time.time() - start))\n",
    "    return X.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'content_img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-2c29a4ee9912>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimage_shape\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34m(\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m225\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcontent_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontents_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_contents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mstyle_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyles_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_styles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-ea7d8a4f3e30>\u001b[0m in \u001b[0;36mget_contents\u001b[0;34m(image_shape)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_contents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mcontent_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_img\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mimage_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mcontents_Y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontent_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontents_Y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'content_img' is not defined"
     ]
    }
   ],
   "source": [
    "image_shape =  (150, 225)\n",
    "content_X, contents_Y = get_contents(image_shape)\n",
    "style_X, styles_Y = get_styles(image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
